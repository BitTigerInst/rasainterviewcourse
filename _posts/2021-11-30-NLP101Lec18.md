## 基于Transformer端到端的任务对话系统解密

一、概述

        本文围绕下面这篇论文来分析面向任务的对话系统与用户是如何交互的，SimpleTOD模型的架构特点与内部机制，以及它所使用的训练函数的剖析等。从SimpleTOD内部实现看，是把语言理解(NLU)，对话状态管理(Dialogue State Tracking)，语言生成(NLG)等采用一种端到端的方式统一起来，在基于开源MultiWOZ数据集进行的训练结果表明，采用这种架构获得了很好的表现。SimpleTOD使用了单一的，基于因果关系的语言模型（如Transformer的decoder端，它是根据左侧的信息来预测右侧被遮住的部分）来训练所有子任务如“理解用户输入”，“系统采取的行为”，“系统产生响应”。对于SimpleTOD来说，这些子任务只是这个单一过程里的不同的“步骤”而已，从而把对话的处理变成一个“单序列的预测问题”。这种架构设计思想使SimpleTOD能够充分利用来自预训练的，开放领域的，具有因果关系的语言模型（如GPT-2）的迁移学习。SimpleTOD不仅改善了对会话状态的管理，它也改善了以端到端的方式评估对话系统行为和响应时使用的主要指标，如”inform rate”提升了8.1个百分点，”success rate”提升了9.7个百分点，综合分数提升了7.2个百分点。


————————————————
版权声明：本文为CSDN博主「m0_49380401」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/m0_49380401/article/details/121754906

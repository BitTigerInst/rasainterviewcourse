## 基于Transformer轻量级高效精确的Conversational Representation对话系统ConveRT解密(二)


       本文继续围绕ConveRT这篇论文来介绍如下内容。

三、方法论

    基于Raddit数据的预训练

        为了简化response selection task这项任务的学习，可以先使用通用领域的数据如Raddit来训练对话任务，然后对这个经过了通用领域数据预训练的response selection模型进行微调，通过调整参数来获得具体领域相关的response selection模型，这可以看做是一种迁移学习方式。之所以使用Raddit数据，有以下两个原因：

-它具有有组织的对话结构

-对话数据足够多，包含了727M的input-response pairs（输入和响应配对）

  2. 更紧凑的response selection模型

        ConveRT使用了基于Transformer的更加紧凑的dual encoder架构，dual encoder是指输入部分和响应部分各自使用了一个encoder，而这两个encoder位于同一个网络空间，参见下面的架构图。 ConveRT利用subword级别的表示，Transformer形式的blocks

————————————————
版权声明：本文为CSDN博主「m0_49380401」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/m0_49380401/article/details/121939135

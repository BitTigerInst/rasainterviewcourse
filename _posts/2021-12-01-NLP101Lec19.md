## 基于Transformer的端到端SimpleTOD实验分析


一、概述

        本文围绕SimpleTOD这篇论文从实验的角度来继续分析。

二、实验结论分析

        SimpleTOD在对话系统中采用了一种简单的方式，即把对话状态管理，系统行为和响应生成等三个部分作为模型中的一个输入序列(sequence)来进行训练。SimpleTOD能够直接利用预训练模型如GPT-2来做基于开放领域可用数据的语言理解迁移学习。基于多领域的对话数据集MultiWOZ的实验结果表明，获得预训练的权重信息是必要的，但是为了利用这些权重，需要使用特殊的token来标记用户输入和系统响应，以及在这个序列中与系统的不同子任务相关的部分。在这里谈到的预训练权重，不一定是指大规模预训练的语言模型如BERT所提供的dense embeddings，也可以是word级别的one-hot encoding, 或者基于字符级别n-gram的multi-hot encoding等，可以通过神经网络来集成其它网络或者算法来达到提升性能的目的。特殊token的使用可以把逻辑变成数据，即在序列里的一个token，可以通过特殊token以类似条件表达式的方式来控制逻辑。

       实验也发现了SimpleTOD在多轮对话的长上下文环境里跟踪对话状态的有效性，在有噪声存在(noisy annotations)的情况下，仅使用”greedy decoding”技术就获得了比较好的训练效果。

————————————————
版权声明：本文为CSDN博主「m0_49380401」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/m0_49380401/article/details/121777445

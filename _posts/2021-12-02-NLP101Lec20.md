Gavin老师Transformer直播课感悟 - 基于Retrieval的Fine-grained架构的对话系统


一、概述

        本文围绕下面这篇论文从多个角度来解析如何构建出具有”Fine-grained”架构的，面向任务的对话系统。

        基于”Retrieval”的对话系统在使用如BERT这样的基于Transformer的预训练模型时，显示出很好的表现。所谓”Fine-grained”，就是对整个对话(dialogue)的长上下文进行拆分，以便于可以更精细化地控制对话，具体来讲，可以把下图中左边标签[context]显示的一个长上下文拆分为几个更短的上下文(Short Context)，用这些短上下文来进行训练，同时使用了Target Utterance从话语层面进行更细粒度的训练。对于BERT来说，是使用NSP来预测两个语句是否从语义上“相邻”，或者说是否来自同一篇文章。对于Target Utterance，有三种输入，一种是正常系统产生的response，另外一种是在对话中使用的Random Utterance（譬如与正常话语顺序不同，可以看做是一种负样本，可以用来训练话语的连贯性(cohe

————————————————
版权声明：本文为CSDN博主「m0_49380401」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/m0_49380401/article/details/121800398

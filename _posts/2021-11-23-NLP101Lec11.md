## BERT论文逐行解密：算法、数学、源码


    从论文角度对BERT全方位深度剖析

本文从BERT的一篇经典论文来剖析BERT的内部机制与实现原理，以及BERT在NLP领域具有的优势或者说运用价值（论文的链接地址：https://arxiv.org/pdf/1810.04805.pdf）。

BERT是论文原话” Bidirectional Encoder Representations from Transformers”的首字母的组合。在这里着重强调两点，一是BERT是基于Transformer衍生而来，我们通过比较两者之间的架构，就很容易理解；二是双向注意力机制，而我们知道这个机制是Transformer本身就具有的。从论文标题可以看出BERT是用来做语言理解的，从架构层面看，BERT采用了Transformer架构的左侧部分，即Encoder端。BERT是具有双向注意力机制并运用了多层神经网络的语言表示模型。


————————————————
版权声明：本文为CSDN博主「m0_49380401」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/m0_49380401/article/details/121618351

## 基于Retrieval的具有Fine-grained架构的对话系统(二）


一、Related work介绍

        最近的研究多集中于在基于retrieval的多轮对话系统中，当一个包含多轮对话的上下文被提供时，系统应该如何选择最合适的响应，如使用BERT对上下文序列进行编码，产生一个dense vector，然后把这个vector同一组可选响应的矩阵进行相乘，比较它们的相关度，然后使用softmax得到概率分布，从而选出一个概率最高的作为系统的响应。在对比这些研究时发现，有一种方式是使用称为IRC语料库的基准数据集和一个基于RNN网络的模型，另一种方式是使用一种基于dual encoder的模型来试图有效地对上下文进行编码和使用LSTM和CNN作为encoder对响应部分进行编码，这里提到的dual encoder可以看做是有左右两个encoder部分，使用左侧对上下文进行编码，而使用右侧对响应部分进行编码，上下文是指当前用户和系统交互的内容，通过编码形成一个dense vector。随着注意力机制的出现，注意力机制被用于对话系统来选择系统响应。譬如通过对话中的多个交互blocks来在上下文和响应之间进行一种深度的交互，从而通过对话状态控制器来改善训练表现。

        论文提到使用开源的BERT模型，具有12层，12个注意力头，768维度的hidden state。BERT有两个训练目标：MLM和NSP，MLM使用掩码机制来进行预测，而NSP是针对给定的两个文本序列A和B，训练模型来决定序列B是否在序列A之后（指位置是否“相邻”），模型把A和B作为输入并使用token [SEP] 进行分隔，然后使用segment embedding 的

————————————————
版权声明：本文为CSDN博主「m0_49380401」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/m0_49380401/article/details/121864176

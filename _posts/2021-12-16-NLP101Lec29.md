## 基于Transformer的Poly-Encoder架构体系解密(一）第29课

NLP高手之路Kernel版101课 第29课 
北京时间今晚21点第29课-周四-12月16号：基于Transformer的Poly-Encoder架构体系解密

1，基于Transformer的Bi-encoder解析 2，基于Transformer的Cross-encoder解析 3，基于Transformer的Poly-encoder解析

coveRT的code https://github.com/duanzhihua/ConveRT-pytorch

conveRT论文代码pytorch复现

一、概述

        本文围绕下面这篇论文来解析基于Transformer采用Poly-Encoder的架构体系如何来平衡模型的速度和质量表现，以及它与其它encoders架构的对比等。

关于速度，譬如在线推理时，如果用户输入后系统需要较长的时间来给出回应，那么在实际运用中是无法接受的，而模型的质量好坏会影响到对话系统的表现，如基于用户输入，系统不能选择正确的回应等。对于需要在两个输入序列(sequence)之间做出成对比较的任务，即使用输入来匹配相应的label(input-response)，通常有两种做法：

    Cross-encoders，基于input-response pair使用全连接自注意力机制，即input上下文中的tokens和response中的tokens是相互注意，相互作用的。

优点：质量非常好，由于在inpu

————————————————
版权声明：本文为CSDN博主「m0_49380401」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/m0_49380401/article/details/122004082

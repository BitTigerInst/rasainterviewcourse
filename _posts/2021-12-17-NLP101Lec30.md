## 基于Transformer的Poly-Encoder架构体系解密(三）

        本文继续围绕下面这篇论文从试验的角度解析基于Transformer采用Poly-Encoder的架构是如何来平衡模型的速度和质量表现，以及试验中在模型表现和推理时间等方面Poly-encoder与Bi-encoder，Cross-encoder架构的对比等。

五、试验部分

    关于如何选择context vectors

        如下图所示，有以下几种方法来从最基本的Transformer的输出(h 1 ctxt, ..., h N ctxt)推导出context vectors(y 1 ctxt, ..., y m ctxt)：

-对m个code(c1,…,cm)进行学习，如上下文ci通过注意力机制和所有输出(h 1 ctxt, ..., h N ctx

————————————————
版权声明：本文为CSDN博主「m0_49380401」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/m0_49380401/article/details/122029881

## 基于Transformer轻量级高效精确的Conversational Representation对话系统ConveRT解密(三)



        本文继续围绕ConveRT这篇论文来介绍如下内容。

三、方法论

        2.4 量化过程

        近期的一些研究表明语言大模型可以通过运用量化技术来达到结构更紧凑的目的，简单来说就是模型的尺寸更小，譬如基于Transformer的机器翻译系统量化后的版本，以及BERT的版本现在都是可用的。在这里，ConveRT针对response selection任务运用了量化相关的对话预训练，试验表明基于下图中dual encoder架构的ConveRT模型可以通过相关量化技术进行训练。

        不同于标准的32位表示的参数，在ConveRT里，所有的embedding参数只使用8位来表示，其它的网络参数只使用16位来表示，通过采用混合精度训练方案(scheme)以一种量化的方式来训练这些参数。每一个具有32位浮点精度的变量会保留一份拷贝，但是模型在训练和推理时对这些变量只使用16位浮点精度。然而
开通VIP 解锁文章 
————————————————
版权声明：本文为CSDN博主「m0_49380401」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/m0_49380401/article/details/121960704

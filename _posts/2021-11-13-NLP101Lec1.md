## Transformer基于Bayesian思想拥抱数据的不确定性


        Transformer基于Encoder-Decoder的架构、Multi-head注意力机制、Dropout和残差网络等都是Bayesian神经网络的具体实现:

       上图中左边的Multi-Head  Attention是指"Encoder self-attention", 通过Encoder的states来计算queries, keys, values, 然后由前馈神经网络来进行处理。右边的"Masked Multi-Head Attention"是指"Decoder self-attention(masked)", 通过Decoder的states来计算queries, keys, values，结果输入到上一层的Multi-Head  Attention，即"Decoder-encoder attention", 这里的queries通过decoder的states获得，而keys 和values 则通过encoder的states获得，之后由前馈神经网络来进行处理。

       基于Transformer各种模型变种及实践也都是基于Bayesian思想指导下来应对数据的不确定性；混合使用各种类型的Embeddings来提供更好Prior信息其实是应用Bayesian思想来集成处理信息表达的不确定性。
————————————————
版权声明：本文为CSDN博主「m0_49380401」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/m0_49380401/article/details/121433257

## 使用Transformer构建具有抗干扰能力的Task-oriented对话系统


一、概述

        本文围绕Dialog Transformers这篇论文来阐述如何使用Transformer建立抗干扰的对话系统。

        首先论文提到了Transformer的自注意力机制(self-attention)可以帮助很好地处理对话中的序列，所谓序列(sequence)，就是在一个用户和系统的对话里，当用户说一句话，之后由系统给出对应的回复，在一个对话里，可能会有很多这样的交互过程，每一次交互就是一个turn，在一个对话任务里往往会包括多个turn。在Transformer中，无论是用户的输入还是系统的输入，都可以转换为文本的sequence，通过在sequence里加入特殊的token用于区分哪些是用户输入，哪些是系统的输入。在一个对话环境里，用户可以在当前的上下文里插入其它的主题，那这样的主题可以看做是当前对话的干扰因素。Transformer可以通过自注意力机制来决定使用哪些会

————————————————
版权声明：本文为CSDN博主「m0_49380401」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/m0_49380401/article/details/121663320

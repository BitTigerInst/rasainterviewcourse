## BERT预训练代码解析


一、BERT模型架构简述

        BERT使用了Transformer架构的Encoder端，它的内部是多层Transformer的Encoder, 在输入部分，包含了内容编码，位置编码，segment编码等信息，输出1到多个hidden vector，然后通过一些线性转换算法来做分类，也可以把输出结果交给其它的模型（如回归分类模型）来进行处理。

         

 

二、数据准备

        

         可以使用很多方式获取数据来源，譬如可以从以下链接下载训练需要的数据：

数据加载之后，需要做一些配置，如指定训练集，测试集，词库的路径，hidden vector的维度，

————————————————
版权声明：本文为CSDN博主「m0_49380401」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/m0_49380401/article/details/121504916

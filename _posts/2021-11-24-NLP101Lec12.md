## 基于Transformer的多轮对话机器人四要素解密

一．概述

        本文通过一篇论文来剖析基于Transformer的面向多轮对话任务的基本的实现原理和4个关键要素，从Transformer的角度分析这些要素是如何衔接起来的，训练目标函数背后的数学机制，以及训练后的效果。论文谈到的面向任务的对话和面向业务的对话实际上是有很大差别的，如在语言理解的力度问题上，用户与业务系统的对话，会有不同的表达方式，譬如一个语句可能有多元的信息或者说多种意图，在处理之后，针对后续实施的行为如用户订餐/订机票等，完成之后如何通知到用户等都需要考虑到，在这个过程中，复杂之处在于对状态的管理。

        从语言模式底层来说，在通用领域任务和面向具体对话任务之间是有很大的区别，这就导致一般的预训练语言模型在对话领域不太有用。为了在预训练中获得更好的对话行为，TOD-BERT语言模型加入了user和system两个特殊的token。如果要调整结构，除了调整模型本身以外，还可以调整模型内部输入的token，

————————————————
版权声明：本文为CSDN博主「m0_49380401」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/m0_49380401/article/details/121641309

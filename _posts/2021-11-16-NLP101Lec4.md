## BERT语言模型内部机制及预训练解析


一、BERT模型概述

        在处理语言时，几乎所有的场景都可以认为是分类场景，所以从这个角度来说，BERT可以用于任何需要分类的场景。BERT使用命名实体识别（NER）来对组成语句的词汇、词组或者短语进行分类，对于1个NER任务，输出使用了token而不是[CLS]。  在问答场景下，问题部分和答案部分使用分隔符[SEP]进行连接，而答案有开始和结束注释。我们可以直接使用BERT模型或者可以根据需要修改它的结构或者模型实现。

         BERT是一个基于自编码的语言分类模型，依赖于原生Transformer架构的左侧部分（encoder）,可以高效解决语言分类问题。从输入到输出的过程可以看做是一个语言编码和解码的过程，然后基于Encoder和Decoder的多层次神经网络输出结果，输出结果产生的误差，可以通过反向传播来调整网络参数，从而提供训练效果：


————————————————
版权声明：本文为CSDN博主「m0_49380401」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/m0_49380401/article/details/121483136

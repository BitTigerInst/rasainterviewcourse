## BERT多任务Fine-tuning案例实战

一、概述

        从任务的视角来看BERT的微调(Fine-tuning), 通过下游监督式学习的任务（设定预定义的标签）来对BERT预训练之后的网络进行微调，这里需要重点关注的是"Further Pre-training"部分和"Multi-Task Fine-tuning"部分。BERT在工作时是个分类器，无论针对的是单任务还是多任务，是聚焦于全局的信息([CLS])还是局部的信息（NER），BERT能够捕获领域通用的语义信息，从而能够更精细化地表达输入内容，在输出时使用vector或者matrix，所以很容易对输出结果进行处理，从而导致BERT可以和传统的机器学习算法相结合，把BERT作为整个训练或推理的一个阶段或者使用BERT作为上游的输入。从下图看，首先BERT会进行一个初步的训练，这个训练可能使用的是比较通用的数据集，然后使用领域相关的数据集进行进一步的训练，所以在前面训练的基础上进行了状态的更新，状态更新意味着MAP的运用，然后进入流水线式的单个任务或者多个任务的微调(Fine-tuning)的过程。

对于文本分类任务，BERT使用[CLS]最后产出的hidden state来表达整个语句的全局的信息，这是因为BERT网络的最后一层表达了更抽象的信息如语义级别的信息。在下面的公式里ÿ

————————————————
版权声明：本文为CSDN博主「m0_49380401」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/m0_49380401/article/details/121569747
